---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21       |
|    ep_rew_mean     | 0.0742   |
| time/              |          |
|    fps             | 222      |
|    iterations      | 1        |
|    time_elapsed    | 9        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21          |
|    ep_rew_mean          | 0.143       |
| time/                   |             |
|    fps                  | 194         |
|    iterations           | 2           |
|    time_elapsed         | 21          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.015846135 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.69       |
|    explained_variance   | -0.548      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.123       |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0298     |
|    std                  | 1           |
|    value_loss           | 0.669       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21         |
|    ep_rew_mean          | 0.261      |
| time/                   |            |
|    fps                  | 197        |
|    iterations           | 3          |
|    time_elapsed         | 31         |
|    total_timesteps      | 6144       |
| train/                  |            |
|    approx_kl            | 0.01767471 |
|    clip_fraction        | 0.163      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.68      |
|    explained_variance   | -0.0897    |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0301     |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.0417    |
|    std                  | 1          |
|    value_loss           | 0.55       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21          |
|    ep_rew_mean          | 0.365       |
| time/                   |             |
|    fps                  | 198         |
|    iterations           | 4           |
|    time_elapsed         | 41          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.019945323 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.66       |
|    explained_variance   | -0.169      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0199      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0507     |
|    std                  | 0.993       |
|    value_loss           | 0.473       |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=1.78 +/- 0.43
Episode length: 21.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 21          |
|    mean_reward          | 1.78        |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.020089274 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.65       |
|    explained_variance   | -0.102      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.028       |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0532     |
|    std                  | 0.991       |
|    value_loss           | 0.526       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21       |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 194      |
|    iterations      | 5        |
|    time_elapsed    | 52       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21          |
|    ep_rew_mean          | 0.643       |
| time/                   |             |
|    fps                  | 196         |
|    iterations           | 6           |
|    time_elapsed         | 62          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.023003206 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.63       |
|    explained_variance   | -0.0225     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00644     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0595     |
|    std                  | 0.986       |
|    value_loss           | 0.451       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21          |
|    ep_rew_mean          | 0.688       |
| time/                   |             |
|    fps                  | 194         |
|    iterations           | 7           |
|    time_elapsed         | 73          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.024044601 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.6        |
|    explained_variance   | 0.0426      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0348      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.057      |
|    std                  | 0.976       |
|    value_loss           | 0.482       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21          |
|    ep_rew_mean          | 0.838       |
| time/                   |             |
|    fps                  | 196         |
|    iterations           | 8           |
|    time_elapsed         | 83          |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.027817592 |
|    clip_fraction        | 0.278       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.55       |
|    explained_variance   | 0.204       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0199      |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0609     |
|    std                  | 0.963       |
|    value_loss           | 0.398       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21         |
|    ep_rew_mean          | 0.893      |
| time/                   |            |
|    fps                  | 198        |
|    iterations           | 9          |
|    time_elapsed         | 93         |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.02893266 |
|    clip_fraction        | 0.299      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.51      |
|    explained_variance   | 0.278      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0544     |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0658    |
|    std                  | 0.955      |
|    value_loss           | 0.4        |
----------------------------------------
Eval num_timesteps=20000, episode_reward=2.29 +/- 0.17
Episode length: 21.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 21          |
|    mean_reward          | 2.29        |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.026459392 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.47       |
|    explained_variance   | 0.401       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00856     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0619     |
|    std                  | 0.945       |
|    value_loss           | 0.291       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21       |
|    ep_rew_mean     | 1.06     |
| time/              |          |
|    fps             | 198      |
|    iterations      | 10       |
|    time_elapsed    | 103      |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21          |
|    ep_rew_mean          | 1.15        |
| time/                   |             |
|    fps                  | 199         |
|    iterations           | 11          |
|    time_elapsed         | 112         |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.027341709 |
|    clip_fraction        | 0.279       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.42       |
|    explained_variance   | 0.49        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0496     |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0624     |
|    std                  | 0.932       |
|    value_loss           | 0.239       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21          |
|    ep_rew_mean          | 1.27        |
| time/                   |             |
|    fps                  | 199         |
|    iterations           | 12          |
|    time_elapsed         | 123         |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.027343754 |
|    clip_fraction        | 0.279       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.36       |
|    explained_variance   | 0.602       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0707     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0642     |
|    std                  | 0.918       |
|    value_loss           | 0.225       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21         |
|    ep_rew_mean          | 1.4        |
| time/                   |            |
|    fps                  | 200        |
|    iterations           | 13         |
|    time_elapsed         | 132        |
|    total_timesteps      | 26624      |
| train/                  |            |
|    approx_kl            | 0.03063757 |
|    clip_fraction        | 0.306      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.3       |
|    explained_variance   | 0.694      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0807    |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0677    |
|    std                  | 0.905      |
|    value_loss           | 0.197      |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 21        |
|    ep_rew_mean          | 1.51      |
| time/                   |           |
|    fps                  | 201       |
|    iterations           | 14        |
|    time_elapsed         | 142       |
|    total_timesteps      | 28672     |
| train/                  |           |
|    approx_kl            | 0.0272935 |
|    clip_fraction        | 0.289     |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.24     |
|    explained_variance   | 0.735     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.0882   |
|    n_updates            | 130       |
|    policy_gradient_loss | -0.0658   |
|    std                  | 0.893     |
|    value_loss           | 0.17      |
---------------------------------------
Eval num_timesteps=30000, episode_reward=2.26 +/- 0.18
Episode length: 21.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 21          |
|    mean_reward          | 2.26        |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.032137346 |
|    clip_fraction        | 0.301       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.19       |
|    explained_variance   | 0.734       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.11       |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0682     |
|    std                  | 0.878       |
|    value_loss           | 0.181       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21       |
|    ep_rew_mean     | 1.52     |
| time/              |          |
|    fps             | 201      |
|    iterations      | 15       |
|    time_elapsed    | 152      |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21          |
|    ep_rew_mean          | 1.61        |
| time/                   |             |
|    fps                  | 200         |
|    iterations           | 16          |
|    time_elapsed         | 163         |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.036333524 |
|    clip_fraction        | 0.327       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.12       |
|    explained_variance   | 0.794       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.086      |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0704     |
|    std                  | 0.864       |
|    value_loss           | 0.142       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21          |
|    ep_rew_mean          | 1.71        |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 17          |
|    time_elapsed         | 172         |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.033155706 |
|    clip_fraction        | 0.315       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.07       |
|    explained_variance   | 0.814       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.107      |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0693     |
|    std                  | 0.857       |
|    value_loss           | 0.123       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21         |
|    ep_rew_mean          | 1.78       |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 18         |
|    time_elapsed         | 182        |
|    total_timesteps      | 36864      |
| train/                  |            |
|    approx_kl            | 0.03063719 |
|    clip_fraction        | 0.288      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.02      |
|    explained_variance   | 0.854      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.125     |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.0646    |
|    std                  | 0.842      |
|    value_loss           | 0.106      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21          |
|    ep_rew_mean          | 1.82        |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 19          |
|    time_elapsed         | 192         |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.032359138 |
|    clip_fraction        | 0.305       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.95       |
|    explained_variance   | 0.863       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.118      |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0674     |
|    std                  | 0.828       |
|    value_loss           | 0.099       |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=2.48 +/- 0.09
Episode length: 21.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 21          |
|    mean_reward          | 2.48        |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.033757586 |
|    clip_fraction        | 0.328       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.89       |
|    explained_variance   | 0.849       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.113      |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0682     |
|    std                  | 0.816       |
|    value_loss           | 0.107       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21       |
|    ep_rew_mean     | 1.87     |
| time/              |          |
|    fps             | 200      |
|    iterations      | 20       |
|    time_elapsed    | 203      |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21          |
|    ep_rew_mean          | 1.92        |
| time/                   |             |
|    fps                  | 200         |
|    iterations           | 21          |
|    time_elapsed         | 213         |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.032534774 |
|    clip_fraction        | 0.307       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.83       |
|    explained_variance   | 0.883       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0944     |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0673     |
|    std                  | 0.804       |
|    value_loss           | 0.0842      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21          |
|    ep_rew_mean          | 1.95        |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 22          |
|    time_elapsed         | 224         |
|    total_timesteps      | 45056       |
| train/                  |             |
|    approx_kl            | 0.033853445 |
|    clip_fraction        | 0.306       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.77       |
|    explained_variance   | 0.887       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.111      |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0662     |
|    std                  | 0.794       |
|    value_loss           | 0.0814      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21         |
|    ep_rew_mean          | 2.02       |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 23         |
|    time_elapsed         | 233        |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.03368054 |
|    clip_fraction        | 0.31       |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.7       |
|    explained_variance   | 0.891      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.134     |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0684    |
|    std                  | 0.776      |
|    value_loss           | 0.074      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21          |
|    ep_rew_mean          | 2.03        |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 24          |
|    time_elapsed         | 243         |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.033410385 |
|    clip_fraction        | 0.339       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.61       |
|    explained_variance   | 0.907       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.132      |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0737     |
|    std                  | 0.758       |
|    value_loss           | 0.0627      |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=2.52 +/- 0.10
Episode length: 21.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 21          |
|    mean_reward          | 2.52        |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.036872607 |
|    clip_fraction        | 0.317       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.54       |
|    explained_variance   | 0.898       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.115      |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0703     |
|    std                  | 0.747       |
|    value_loss           | 0.0668      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21       |
|    ep_rew_mean     | 2.11     |
| time/              |          |
|    fps             | 201      |
|    iterations      | 25       |
|    time_elapsed    | 253      |
|    total_timesteps | 51200    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21         |
|    ep_rew_mean          | 2.12       |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 26         |
|    time_elapsed         | 263        |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.04058396 |
|    clip_fraction        | 0.332      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.46      |
|    explained_variance   | 0.92       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.143     |
|    n_updates            | 250        |
|    policy_gradient_loss | -0.0714    |
|    std                  | 0.732      |
|    value_loss           | 0.055      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21          |
|    ep_rew_mean          | 2.19        |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 27          |
|    time_elapsed         | 273         |
|    total_timesteps      | 55296       |
| train/                  |             |
|    approx_kl            | 0.038995758 |
|    clip_fraction        | 0.341       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.39       |
|    explained_variance   | 0.925       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.126      |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0737     |
|    std                  | 0.721       |
|    value_loss           | 0.0499      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21          |
|    ep_rew_mean          | 2.26        |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 28          |
|    time_elapsed         | 283         |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.042571366 |
|    clip_fraction        | 0.357       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.32       |
|    explained_variance   | 0.922       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.119      |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0742     |
|    std                  | 0.708       |
|    value_loss           | 0.0522      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21          |
|    ep_rew_mean          | 2.28        |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 29          |
|    time_elapsed         | 293         |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.039446965 |
|    clip_fraction        | 0.333       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.138      |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0691     |
|    std                  | 0.699       |
|    value_loss           | 0.0452      |
-----------------------------------------
Eval num_timesteps=60000, episode_reward=2.66 +/- 0.06
Episode length: 21.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 21         |
|    mean_reward          | 2.66       |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.03499929 |
|    clip_fraction        | 0.321      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.21      |
|    explained_variance   | 0.936      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.12      |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.0702    |
|    std                  | 0.691      |
|    value_loss           | 0.0446     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21       |
|    ep_rew_mean     | 2.29     |
| time/              |          |
|    fps             | 202      |
|    iterations      | 30       |
|    time_elapsed    | 302      |
|    total_timesteps | 61440    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21          |
|    ep_rew_mean          | 2.32        |
| time/                   |             |
|    fps                  | 203         |
|    iterations           | 31          |
|    time_elapsed         | 312         |
|    total_timesteps      | 63488       |
| train/                  |             |
|    approx_kl            | 0.041640352 |
|    clip_fraction        | 0.344       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.15       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.14       |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.0729     |
|    std                  | 0.677       |
|    value_loss           | 0.0402      |
-----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 21        |
|    ep_rew_mean          | 2.33      |
| time/                   |           |
|    fps                  | 203       |
|    iterations           | 32        |
|    time_elapsed         | 322       |
|    total_timesteps      | 65536     |
| train/                  |           |
|    approx_kl            | 0.0413965 |
|    clip_fraction        | 0.346     |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.07     |
|    explained_variance   | 0.949     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.142    |
|    n_updates            | 310       |
|    policy_gradient_loss | -0.0711   |
|    std                  | 0.664     |
|    value_loss           | 0.0351    |
---------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21          |
|    ep_rew_mean          | 2.36        |
| time/                   |             |
|    fps                  | 203         |
|    iterations           | 33          |
|    time_elapsed         | 332         |
|    total_timesteps      | 67584       |
| train/                  |             |
|    approx_kl            | 0.045533683 |
|    clip_fraction        | 0.358       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.02       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.11       |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.073      |
|    std                  | 0.657       |
|    value_loss           | 0.0389      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21         |
|    ep_rew_mean          | 2.36       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 34         |
|    time_elapsed         | 342        |
|    total_timesteps      | 69632      |
| train/                  |            |
|    approx_kl            | 0.04280337 |
|    clip_fraction        | 0.345      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.98      |
|    explained_variance   | 0.951      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.12      |
|    n_updates            | 330        |
|    policy_gradient_loss | -0.0734    |
|    std                  | 0.653      |
|    value_loss           | 0.034      |
----------------------------------------
Eval num_timesteps=70000, episode_reward=2.67 +/- 0.04
Episode length: 21.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 21         |
|    mean_reward          | 2.67       |
| time/                   |            |
|    total_timesteps      | 70000      |
| train/                  |            |
|    approx_kl            | 0.04419258 |
|    clip_fraction        | 0.351      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.94      |
|    explained_variance   | 0.943      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.142     |
|    n_updates            | 340        |
|    policy_gradient_loss | -0.0727    |
|    std                  | 0.644      |
|    value_loss           | 0.0349     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21       |
|    ep_rew_mean     | 2.37     |
| time/              |          |
|    fps             | 202      |
|    iterations      | 35       |
|    time_elapsed    | 353      |
|    total_timesteps | 71680    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21          |
|    ep_rew_mean          | 2.42        |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 36          |
|    time_elapsed         | 365         |
|    total_timesteps      | 73728       |
| train/                  |             |
|    approx_kl            | 0.043846704 |
|    clip_fraction        | 0.341       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.87       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.153      |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.0748     |
|    std                  | 0.632       |
|    value_loss           | 0.0302      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21         |
|    ep_rew_mean          | 2.4        |
| time/                   |            |
|    fps                  | 200        |
|    iterations           | 37         |
|    time_elapsed         | 377        |
|    total_timesteps      | 75776      |
| train/                  |            |
|    approx_kl            | 0.04060444 |
|    clip_fraction        | 0.34       |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.81      |
|    explained_variance   | 0.956      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.112     |
|    n_updates            | 360        |
|    policy_gradient_loss | -0.0716    |
|    std                  | 0.623      |
|    value_loss           | 0.0305     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21         |
|    ep_rew_mean          | 2.43       |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 38         |
|    time_elapsed         | 387        |
|    total_timesteps      | 77824      |
| train/                  |            |
|    approx_kl            | 0.04664022 |
|    clip_fraction        | 0.366      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.75      |
|    explained_variance   | 0.951      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.129     |
|    n_updates            | 370        |
|    policy_gradient_loss | -0.0727    |
|    std                  | 0.613      |
|    value_loss           | 0.0311     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21          |
|    ep_rew_mean          | 2.44        |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 39          |
|    time_elapsed         | 397         |
|    total_timesteps      | 79872       |
| train/                  |             |
|    approx_kl            | 0.045009032 |
|    clip_fraction        | 0.348       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.68       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.129      |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.0738     |
|    std                  | 0.603       |
|    value_loss           | 0.0292      |
-----------------------------------------
Eval num_timesteps=80000, episode_reward=2.69 +/- 0.02
Episode length: 21.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 21          |
|    mean_reward          | 2.69        |
| time/                   |             |
|    total_timesteps      | 80000       |
| train/                  |             |
|    approx_kl            | 0.050203532 |
|    clip_fraction        | 0.377       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.63       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.123      |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.0764     |
|    std                  | 0.597       |
|    value_loss           | 0.0255      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21       |
|    ep_rew_mean     | 2.45     |
| time/              |          |
|    fps             | 200      |
|    iterations      | 40       |
|    time_elapsed    | 407      |
|    total_timesteps | 81920    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21         |
|    ep_rew_mean          | 2.46       |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 41         |
|    time_elapsed         | 417        |
|    total_timesteps      | 83968      |
| train/                  |            |
|    approx_kl            | 0.04904195 |
|    clip_fraction        | 0.368      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.58      |
|    explained_variance   | 0.962      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.132     |
|    n_updates            | 400        |
|    policy_gradient_loss | -0.074     |
|    std                  | 0.588      |
|    value_loss           | 0.024      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21         |
|    ep_rew_mean          | 2.51       |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 42         |
|    time_elapsed         | 427        |
|    total_timesteps      | 86016      |
| train/                  |            |
|    approx_kl            | 0.04899512 |
|    clip_fraction        | 0.384      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.54      |
|    explained_variance   | 0.96       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.127     |
|    n_updates            | 410        |
|    policy_gradient_loss | -0.0764    |
|    std                  | 0.586      |
|    value_loss           | 0.0246     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21         |
|    ep_rew_mean          | 2.49       |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 43         |
|    time_elapsed         | 437        |
|    total_timesteps      | 88064      |
| train/                  |            |
|    approx_kl            | 0.05305285 |
|    clip_fraction        | 0.365      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.5       |
|    explained_variance   | 0.962      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.147     |
|    n_updates            | 420        |
|    policy_gradient_loss | -0.076     |
|    std                  | 0.577      |
|    value_loss           | 0.0254     |
----------------------------------------
Eval num_timesteps=90000, episode_reward=2.70 +/- 0.01
Episode length: 21.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 21         |
|    mean_reward          | 2.7        |
| time/                   |            |
|    total_timesteps      | 90000      |
| train/                  |            |
|    approx_kl            | 0.05186661 |
|    clip_fraction        | 0.385      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.46      |
|    explained_variance   | 0.96       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.14      |
|    n_updates            | 430        |
|    policy_gradient_loss | -0.0766    |
|    std                  | 0.574      |
|    value_loss           | 0.0256     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21       |
|    ep_rew_mean     | 2.52     |
| time/              |          |
|    fps             | 201      |
|    iterations      | 44       |
|    time_elapsed    | 448      |
|    total_timesteps | 90112    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21          |
|    ep_rew_mean          | 2.51        |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 45          |
|    time_elapsed         | 457         |
|    total_timesteps      | 92160       |
| train/                  |             |
|    approx_kl            | 0.052303847 |
|    clip_fraction        | 0.392       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.42       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.118      |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.0753     |
|    std                  | 0.568       |
|    value_loss           | 0.0217      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21          |
|    ep_rew_mean          | 2.54        |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 46          |
|    time_elapsed         | 467         |
|    total_timesteps      | 94208       |
| train/                  |             |
|    approx_kl            | 0.053722598 |
|    clip_fraction        | 0.402       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.39       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.119      |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.0768     |
|    std                  | 0.563       |
|    value_loss           | 0.0215      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21          |
|    ep_rew_mean          | 2.53        |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 47          |
|    time_elapsed         | 476         |
|    total_timesteps      | 96256       |
| train/                  |             |
|    approx_kl            | 0.049674153 |
|    clip_fraction        | 0.377       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.34       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.145      |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.0759     |
|    std                  | 0.555       |
|    value_loss           | 0.019       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21          |
|    ep_rew_mean          | 2.53        |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 48          |
|    time_elapsed         | 485         |
|    total_timesteps      | 98304       |
| train/                  |             |
|    approx_kl            | 0.057954125 |
|    clip_fraction        | 0.394       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.3        |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.144      |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.0789     |
|    std                  | 0.549       |
|    value_loss           | 0.0236      |
-----------------------------------------
Eval num_timesteps=100000, episode_reward=2.72 +/- 0.02
Episode length: 21.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 21         |
|    mean_reward          | 2.72       |
| time/                   |            |
|    total_timesteps      | 100000     |
| train/                  |            |
|    approx_kl            | 0.05185929 |
|    clip_fraction        | 0.388      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.26      |
|    explained_variance   | 0.969      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.136     |
|    n_updates            | 480        |
|    policy_gradient_loss | -0.0773    |
|    std                  | 0.544      |
|    value_loss           | 0.0202     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21       |
|    ep_rew_mean     | 2.55     |
| time/              |          |
|    fps             | 202      |
|    iterations      | 49       |
|    time_elapsed    | 496      |
|    total_timesteps | 100352   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21          |
|    ep_rew_mean          | 2.54        |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 50          |
|    time_elapsed         | 506         |
|    total_timesteps      | 102400      |
| train/                  |             |
|    approx_kl            | 0.052088138 |
|    clip_fraction        | 0.372       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.22       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.135      |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.0753     |
|    std                  | 0.538       |
|    value_loss           | 0.0196      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21         |
|    ep_rew_mean          | 2.57       |
| time/                   |            |
|    fps                  | 202        |
|    iterations           | 51         |
|    time_elapsed         | 516        |
|    total_timesteps      | 104448     |
| train/                  |            |
|    approx_kl            | 0.05750306 |
|    clip_fraction        | 0.404      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.17      |
|    explained_variance   | 0.972      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.154     |
|    n_updates            | 500        |
|    policy_gradient_loss | -0.0789    |
|    std                  | 0.532      |
|    value_loss           | 0.0167     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21         |
|    ep_rew_mean          | 2.56       |
| time/                   |            |
|    fps                  | 202        |
|    iterations           | 52         |
|    time_elapsed         | 526        |
|    total_timesteps      | 106496     |
| train/                  |            |
|    approx_kl            | 0.05953212 |
|    clip_fraction        | 0.405      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.12      |
|    explained_variance   | 0.965      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.125     |
|    n_updates            | 510        |
|    policy_gradient_loss | -0.0744    |
|    std                  | 0.526      |
|    value_loss           | 0.0203     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21          |
|    ep_rew_mean          | 2.56        |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 53          |
|    time_elapsed         | 536         |
|    total_timesteps      | 108544      |
| train/                  |             |
|    approx_kl            | 0.054978974 |
|    clip_fraction        | 0.42        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.08       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.135      |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.077      |
|    std                  | 0.52        |
|    value_loss           | 0.0174      |
-----------------------------------------
Eval num_timesteps=110000, episode_reward=2.70 +/- 0.04
Episode length: 21.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 21         |
|    mean_reward          | 2.7        |
| time/                   |            |
|    total_timesteps      | 110000     |
| train/                  |            |
|    approx_kl            | 0.06712463 |
|    clip_fraction        | 0.43       |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.03      |
|    explained_variance   | 0.974      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.133     |
|    n_updates            | 530        |
|    policy_gradient_loss | -0.0804    |
|    std                  | 0.515      |
|    value_loss           | 0.0168     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21       |
|    ep_rew_mean     | 2.58     |
| time/              |          |
|    fps             | 202      |
|    iterations      | 54       |
|    time_elapsed    | 547      |
|    total_timesteps | 110592   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21          |
|    ep_rew_mean          | 2.58        |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 55          |
|    time_elapsed         | 557         |
|    total_timesteps      | 112640      |
| train/                  |             |
|    approx_kl            | 0.055857293 |
|    clip_fraction        | 0.422       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3          |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.139      |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.0787     |
|    std                  | 0.512       |
|    value_loss           | 0.0162      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21         |
|    ep_rew_mean          | 2.56       |
| time/                   |            |
|    fps                  | 202        |
|    iterations           | 56         |
|    time_elapsed         | 567        |
|    total_timesteps      | 114688     |
| train/                  |            |
|    approx_kl            | 0.05866328 |
|    clip_fraction        | 0.4        |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.99      |
|    explained_variance   | 0.966      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.14      |
|    n_updates            | 550        |
|    policy_gradient_loss | -0.0737    |
|    std                  | 0.51       |
|    value_loss           | 0.0209     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21          |
|    ep_rew_mean          | 2.58        |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 57          |
|    time_elapsed         | 577         |
|    total_timesteps      | 116736      |
| train/                  |             |
|    approx_kl            | 0.066298544 |
|    clip_fraction        | 0.427       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.98       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.143      |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.0779     |
|    std                  | 0.51        |
|    value_loss           | 0.0172      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21          |
|    ep_rew_mean          | 2.57        |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 58          |
|    time_elapsed         | 587         |
|    total_timesteps      | 118784      |
| train/                  |             |
|    approx_kl            | 0.059005633 |
|    clip_fraction        | 0.412       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.95       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.155      |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.0748     |
|    std                  | 0.505       |
|    value_loss           | 0.02        |
-----------------------------------------
Eval num_timesteps=120000, episode_reward=2.71 +/- 0.01
Episode length: 21.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 21         |
|    mean_reward          | 2.71       |
| time/                   |            |
|    total_timesteps      | 120000     |
| train/                  |            |
|    approx_kl            | 0.06630228 |
|    clip_fraction        | 0.431      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.91      |
|    explained_variance   | 0.973      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.139     |
|    n_updates            | 580        |
|    policy_gradient_loss | -0.0783    |
|    std                  | 0.499      |
|    value_loss           | 0.0161     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21       |
|    ep_rew_mean     | 2.61     |
| time/              |          |
|    fps             | 202      |
|    iterations      | 59       |
|    time_elapsed    | 597      |
|    total_timesteps | 120832   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21          |
|    ep_rew_mean          | 2.58        |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 60          |
|    time_elapsed         | 607         |
|    total_timesteps      | 122880      |
| train/                  |             |
|    approx_kl            | 0.064828426 |
|    clip_fraction        | 0.429       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.88       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.125      |
|    n_updates            | 590         |
|    policy_gradient_loss | -0.0805     |
|    std                  | 0.496       |
|    value_loss           | 0.0169      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21          |
|    ep_rew_mean          | 2.61        |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 61          |
|    time_elapsed         | 617         |
|    total_timesteps      | 124928      |
| train/                  |             |
|    approx_kl            | 0.069316536 |
|    clip_fraction        | 0.438       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.83       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.151      |
|    n_updates            | 600         |
|    policy_gradient_loss | -0.0799     |
|    std                  | 0.489       |
|    value_loss           | 0.018       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21         |
|    ep_rew_mean          | 2.6        |
| time/                   |            |
|    fps                  | 202        |
|    iterations           | 62         |
|    time_elapsed         | 627        |
|    total_timesteps      | 126976     |
| train/                  |            |
|    approx_kl            | 0.06644525 |
|    clip_fraction        | 0.441      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.78      |
|    explained_variance   | 0.971      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.133     |
|    n_updates            | 610        |
|    policy_gradient_loss | -0.079     |
|    std                  | 0.483      |
|    value_loss           | 0.017      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21         |
|    ep_rew_mean          | 2.6        |
| time/                   |            |
|    fps                  | 202        |
|    iterations           | 63         |
|    time_elapsed         | 636        |
|    total_timesteps      | 129024     |
| train/                  |            |
|    approx_kl            | 0.08162564 |
|    clip_fraction        | 0.447      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.74      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.129     |
|    n_updates            | 620        |
|    policy_gradient_loss | -0.0792    |
|    std                  | 0.479      |
|    value_loss           | 0.0146     |
----------------------------------------
Eval num_timesteps=130000, episode_reward=2.69 +/- 0.03
Episode length: 21.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 21          |
|    mean_reward          | 2.69        |
| time/                   |             |
|    total_timesteps      | 130000      |
| train/                  |             |
|    approx_kl            | 0.075100094 |
|    clip_fraction        | 0.453       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.7        |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.131      |
|    n_updates            | 630         |
|    policy_gradient_loss | -0.0827     |
|    std                  | 0.476       |
|    value_loss           | 0.0158      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21       |
|    ep_rew_mean     | 2.62     |
| time/              |          |
|    fps             | 202      |
|    iterations      | 64       |
|    time_elapsed    | 646      |
|    total_timesteps | 131072   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21         |
|    ep_rew_mean          | 2.6        |
| time/                   |            |
|    fps                  | 202        |
|    iterations           | 65         |
|    time_elapsed         | 655        |
|    total_timesteps      | 133120     |
| train/                  |            |
|    approx_kl            | 0.06756325 |
|    clip_fraction        | 0.443      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.69      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.146     |
|    n_updates            | 640        |
|    policy_gradient_loss | -0.081     |
|    std                  | 0.474      |
|    value_loss           | 0.0142     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21          |
|    ep_rew_mean          | 2.63        |
| time/                   |             |
|    fps                  | 203         |
|    iterations           | 66          |
|    time_elapsed         | 665         |
|    total_timesteps      | 135168      |
| train/                  |             |
|    approx_kl            | 0.072950915 |
|    clip_fraction        | 0.434       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.66       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.113      |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.0758     |
|    std                  | 0.469       |
|    value_loss           | 0.0159      |
-----------------------------------------
Traceback (most recent call last):
  File "c:\Users\islam\OneDrive\Belgeler\GitHub\GNNportfolio\portfolio_trainer.py", line 292, in <module>
    print(f"{metric}: {value:.4f}")
    ^^^^^^
  File "c:\Users\islam\OneDrive\Belgeler\GitHub\GNNportfolio\portfolio_trainer.py", line 275, in main
    model_params=model_params,
  File "c:\Users\islam\OneDrive\Belgeler\GitHub\GNNportfolio\portfolio_trainer.py", line 138, in train
    self.model.learn(
  File "C:\Users\islam\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\stable_baselines3\ppo\ppo.py", line 315, in learn
    return super().learn(
           ^^^^^^^^^^^^^^
  File "C:\Users\islam\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 300, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\islam\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 195, in collect_rollouts
    new_obs, rewards, dones, infos = env.step(clipped_actions)
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\islam\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\stable_baselines3\common\vec_env\base_vec_env.py", line 206, in step
    return self.step_wait()
           ^^^^^^^^^^^^^^^^
  File "C:\Users\islam\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\stable_baselines3\common\vec_env\vec_normalize.py", line 181, in step_wait
    obs, rewards, dones, infos = self.venv.step_wait()
                                 ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\islam\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\stable_baselines3\common\vec_env\dummy_vec_env.py", line 58, in step_wait
    obs, self.buf_rews[env_idx], terminated, truncated, self.buf_infos[env_idx] = self.envs[env_idx].step(
                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\islam\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\stable_baselines3\common\monitor.py", line 94, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\islam\OneDrive\Belgeler\GitHub\GNNportfolio\financial_env.py", line 212, in step
    reward = self._calculate_reward(weights)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\islam\OneDrive\Belgeler\GitHub\GNNportfolio\financial_env.py", line 310, in _calculate_reward
    components = self._get_reward_components(weights)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\islam\OneDrive\Belgeler\GitHub\GNNportfolio\financial_env.py", line 344, in _get_reward_components
    components[reward_type] = func(weights)
                              ^^^^^^^^^^^^^
  File "c:\Users\islam\OneDrive\Belgeler\GitHub\GNNportfolio\financial_env.py", line 107, in <lambda>
    RewardFactory._calculate_sortino_ratio(
  File "c:\Users\islam\OneDrive\Belgeler\GitHub\GNNportfolio\financial_env.py", line 140, in _calculate_sortino_ratio
    downside_std = np.std(downside_returns) + 1e-6
                   ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\islam\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\numpy\core\fromnumeric.py", line 3645, in std
    return _methods._std(a, axis=axis, dtype=dtype, out=out, ddof=ddof,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\islam\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\numpy\core\_methods.py", line 212, in _std
    ret = ret.dtype.type(um.sqrt(ret))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
